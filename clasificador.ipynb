{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjHHRsU80_Qu"
      },
      "source": [
        "# Problema de clasificación de imágenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz0Jx3591Dj_"
      },
      "source": [
        "## Preparación del entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KaKK-mi8ObY"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcam"
      ],
      "metadata": {
        "id": "MUyDNgxLF-Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIOQZg7X1WY-"
      },
      "source": [
        "### Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OKbIAAM1R2E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torchcam.methods import GradCAM\n",
        "from torchcam.utils import overlay_mask\n",
        "import random"
      ],
      "metadata": {
        "id": "t9IUVgZ05KXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMjrFiH01bir"
      },
      "source": [
        "### Configuración del dispositivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6IAw5Qo1efH"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU1uRc3E1rPZ"
      },
      "source": [
        "## Preparación del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl_aMkJ613Ti"
      },
      "source": [
        "### Definición de transformaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e7HEjAA1suj"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS4M48jh186r"
      },
      "source": [
        "### Carga del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTJy3pYITpOn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyVaKqCW18YT"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/MyDrive/SIGE-P2/starting-package/data x20\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97afJg7g157R"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.ImageFolder(root=data_path, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZac2OVI6kWh"
      },
      "source": [
        "### División: Entrenamiento y validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk0WhmBe6nwI"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTOwmFeu6qai"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm7c682kTjVb"
      },
      "source": [
        "## Análisis Exploratorio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGKS9nV4UEPP"
      },
      "outputs": [],
      "source": [
        "# Número de clases\n",
        "print(f\"Número de clases: {len(dataset.classes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZrL7oThUOFY"
      },
      "outputs": [],
      "source": [
        "# Número de imágenes\n",
        "print(f\"Número total de imágenes: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHJH0YdMURvu"
      },
      "outputs": [],
      "source": [
        "# Conteo por clase\n",
        "class_counts = Counter([label for _, label in dataset.samples])\n",
        "for i, count in class_counts.items():\n",
        "    print(f\"{dataset.classes[i]}: {count} imágenes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbSFzmloYiVZ"
      },
      "outputs": [],
      "source": [
        "# Limpiar nombres de clases: quitar prefijo numérico y reemplazar guiones bajos\n",
        "def clean_name(class_name):\n",
        "    name_without_prefix = class_name.split('.', 1)[-1]\n",
        "    return name_without_prefix.replace('_', ' ')\n",
        "\n",
        "clean_names = {i: clean_name(name) for i, name in enumerate(dataset.classes)}\n",
        "label_clean_names = [clean_names[label] for _, label in dataset.samples]\n",
        "label_count = Counter(label_clean_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSLroefrVElo"
      },
      "outputs": [],
      "source": [
        "# Distribución por clases\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(label_count.keys(), label_count.values(), color='skyblue')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Distribución de clases\")\n",
        "plt.xlabel(\"Clase\")\n",
        "plt.ylabel(\"Número de imágenes\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQSs-odcWs8i"
      },
      "outputs": [],
      "source": [
        "# Crear diccionario con nombres limpios\n",
        "clean_names = {i: clean_name(name) for i, name in enumerate(dataset.classes)}\n",
        "\n",
        "# Agrupar rutas de imágenes por clase\n",
        "class_images = defaultdict(list)\n",
        "for path, label in dataset.samples:\n",
        "    class_images[label].append(path)\n",
        "\n",
        "# Mostrar una imagen aleatoria por clase\n",
        "num_classes = len(dataset.classes)\n",
        "cols = 5\n",
        "rows = (num_classes + cols - 1) // cols\n",
        "plt.figure(figsize=(15, rows * 3))\n",
        "\n",
        "for label in range(num_classes):\n",
        "    img_path = random.choice(class_images[label])\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    plt.subplot(rows, cols, label + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(clean_names[label], fontsize=9)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om3utQayvsAS"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkT4d_cMvz75"
      },
      "source": [
        "### Funciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z4OgM9pYxzJ"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, val_loader, scheduler, num_epochs=10):\n",
        "    train_loss_history, val_loss_history = [], []\n",
        "    train_acc_history, val_acc_history = [], []\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nÉpoca {epoch + 1}/{num_epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # === ENTRENAMIENTO ===\n",
        "        model.train()\n",
        "        running_loss, running_corrects = 0.0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            preds = outputs.argmax(1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += (preds == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_acc = running_corrects / len(train_loader.dataset)\n",
        "        train_loss_history.append(train_loss)\n",
        "        train_acc_history.append(train_acc)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "        # === VALIDACIÓN ===\n",
        "        model.eval()\n",
        "        val_loss_total, val_corrects = 0.0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                preds = outputs.argmax(1)\n",
        "\n",
        "                val_loss_total += loss.item() * inputs.size(0)\n",
        "                val_corrects += (preds == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss_total / len(val_loader.dataset)\n",
        "        val_acc = val_corrects / len(val_loader.dataset)\n",
        "        val_loss_history.append(val_loss)\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict()\n",
        "\n",
        "    print(f\"\\n✅ Mejor Accuracy de Validación: {best_val_acc:.4f}\")\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model, train_loss_history, val_loss_history, train_acc_history, val_acc_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUCgvOJTY6x_"
      },
      "outputs": [],
      "source": [
        "def plot_training(train_loss, val_loss, train_acc, val_acc):\n",
        "    epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Curva de pérdida\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_loss, label='Entrenamiento')\n",
        "    plt.plot(epochs, val_loss, label='Validación')\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Pérdida')\n",
        "    plt.title('Pérdida durante el entrenamiento')\n",
        "    plt.legend()\n",
        "\n",
        "    # Curva de accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_acc, label='Entrenamiento')\n",
        "    plt.plot(epochs, val_acc, label='Validación')\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy durante el entrenamiento')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqGGwlUAv5UW"
      },
      "source": [
        "### Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR4bUVBrv6kl"
      },
      "source": [
        "#### resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGU8oFKuX3AL"
      },
      "outputs": [],
      "source": [
        "# Cargar modelo preentrenado\n",
        "model18 = models.resnet18(pretrained=True)\n",
        "\n",
        "# Congelar capas convolucionales\n",
        "for param in model18.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Reemplazar la capa final para adaptarla a nuestras clases\n",
        "num_ftrs = model18.fc.in_features\n",
        "model18.fc = nn.Linear(num_ftrs, len(dataset.classes))\n",
        "\n",
        "# Mover el modelo al dispositivo adecuado (GPU/CPU)\n",
        "model18 = model18.to(device)\n",
        "\n",
        "# Definir función de pérdida y optimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model18.fc.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6fLZpRnY9dz"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento del modelo\n",
        "model, train_loss, val_loss, train_acc, val_acc = train_model(\n",
        "    model18, criterion, optimizer, train_loader, val_loader, scheduler, num_epochs=10\n",
        ")\n",
        "\n",
        "# Visualización de resultados\n",
        "plot_training(train_loss, val_loss, train_acc, val_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB9_sDaRp0lp"
      },
      "source": [
        "#### resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjpS58VIp2wk"
      },
      "outputs": [],
      "source": [
        "# Cargar modelo preentrenado\n",
        "model50 = models.resnet50(pretrained=True)\n",
        "\n",
        "# Congelar capas convolucionales\n",
        "for param in model50.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Reemplazar la capa final para adaptarla a nuestras clases\n",
        "num_ftrs = model50.fc.in_features\n",
        "model50.fc = nn.Linear(num_ftrs, len(dataset.classes))\n",
        "\n",
        "# Mover el modelo al dispositivo adecuado (GPU/CPU)\n",
        "model50 = model50.to(device)\n",
        "\n",
        "# Definir función de pérdida y optimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model50.fc.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfPstm78qCuV"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento del modelo\n",
        "model, train_loss, val_loss, train_acc, val_acc = train_model(\n",
        "    model50, criterion, optimizer, train_loader, val_loader, scheduler, num_epochs=10\n",
        ")\n",
        "\n",
        "# Visualización de resultados\n",
        "plot_training(train_loss, val_loss, train_acc, val_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_22zgk8wGV8"
      },
      "source": [
        "## Aumento a 200 categorías"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkfj1keY83ps"
      },
      "source": [
        "### Carga del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0dNs0gtwIwR"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/MyDrive/SIGE-P2/starting-package/data x200\"\n",
        "dataset = datasets.ImageFolder(root=data_path, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HQ7gO6y9gNk"
      },
      "source": [
        "### División: Entrenamiento y validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBCYi3nqwSSY"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLE_WBW1wVLC"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNX0znJD9i0f"
      },
      "source": [
        "### Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wr_CA7z970g"
      },
      "outputs": [],
      "source": [
        "# Cargar modelo preentrenado\n",
        "model50 = models.resnet50(pretrained=True)\n",
        "\n",
        "# Congelar capas convolucionales\n",
        "for param in model50.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Reemplazar la capa final para adaptarla a nuestras clases\n",
        "num_ftrs = model50.fc.in_features\n",
        "model50.fc = nn.Linear(num_ftrs, len(dataset.classes))\n",
        "\n",
        "# Mover el modelo al dispositivo adecuado (GPU/CPU)\n",
        "model50 = model50.to(device)\n",
        "\n",
        "# Definir función de pérdida y optimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model50.fc.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HMSyrAN9-o4"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento del modelo\n",
        "model, train_loss, val_loss, train_acc, val_acc = train_model(\n",
        "    model50, criterion, optimizer, train_loader, val_loader, scheduler, num_epochs=10\n",
        ")\n",
        "\n",
        "# Visualización de resultados\n",
        "plot_training(train_loss, val_loss, train_acc, val_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XloFUMX_6bc0"
      },
      "source": [
        "## Mejoras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXNvqgCe-PKT"
      },
      "source": [
        "### Weights And Biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSYWl1Ki-O2g"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9Edlu0-SG1"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"clasificador-cub200\",\n",
        "    name=\"resnet50-x200\",\n",
        "    config={\n",
        "        \"modelo\": \"resnet50\",\n",
        "        \"fine_tuning\": \"layer4\",\n",
        "        \"clases\": 20,\n",
        "        \"batch_size\": 32,\n",
        "        \"epochs\": 10,\n",
        "        \"optimizer\": \"AdamW\",\n",
        "        \"lr\": 1e-4,\n",
        "        \"early_stopping\": True,\n",
        "        \"label_smoothing\": 0.1,\n",
        "        \"data_augmentation\": True\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb3wLDE07JV1"
      },
      "source": [
        "### Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAWG3y286dp3"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLvnH2b_7Sb4"
      },
      "source": [
        "### Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHSLkthF7NrS"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, val_loader, scheduler=None, num_epochs=10):\n",
        "    train_loss_history, val_loss_history = [], []\n",
        "    train_acc_history, val_acc_history = [], []\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_model_state = None\n",
        "\n",
        "    # Early Stopping\n",
        "    patience = 3\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    wandb.watch(model)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nÉpoca {epoch + 1}/{num_epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # === ENTRENAMIENTO ===\n",
        "        model.train()\n",
        "        running_loss, running_corrects = 0.0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            preds = outputs.argmax(1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += (preds == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_acc = running_corrects / len(train_loader.dataset)\n",
        "        train_loss_history.append(train_loss)\n",
        "        train_acc_history.append(train_acc)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "        # === VALIDACIÓN ===\n",
        "        model.eval()\n",
        "        val_loss_total, val_corrects = 0.0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                preds = outputs.argmax(1)\n",
        "\n",
        "                val_loss_total += loss.item() * inputs.size(0)\n",
        "                val_corrects += (preds == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss_total / len(val_loader.dataset)\n",
        "        val_acc = val_corrects / len(val_loader.dataset)\n",
        "        val_loss_history.append(val_loss)\n",
        "        val_acc_history.append(val_acc)\n",
        "\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Scheduler opcional (ReduceLROnPlateau, etc.)\n",
        "        if scheduler:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "        # === EARLY STOPPING ===\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict()\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            print(f\"🕓 Sin mejora en val_acc durante {epochs_no_improve} épocas\")\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(\"⏹️ Early stopping activado\")\n",
        "                break\n",
        "\n",
        "    print(f\"\\n✅ Mejor Accuracy de Validación: {best_val_acc:.4f}\")\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model, train_loss_history, val_loss_history, train_acc_history, val_acc_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UFY-z_n79CO"
      },
      "source": [
        "### Label Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0Tm1YVE7mOJ"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B89IMU4r-e2_"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blUohbR8-iZ1"
      },
      "outputs": [],
      "source": [
        "# Cargar modelo preentrenado\n",
        "model50 = models.resnet50(pretrained=True)\n",
        "\n",
        "# Congelar capas convolucionales\n",
        "for param in model50.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Reemplazar la capa final para adaptarla a nuestras clases\n",
        "num_ftrs = model50.fc.in_features\n",
        "model50.fc = nn.Linear(num_ftrs, len(dataset.classes))\n",
        "\n",
        "# Mover el modelo al dispositivo adecuado (GPU/CPU)\n",
        "model50 = model50.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model50.fc.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1l0-H4O-lbl"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento del modelo\n",
        "model, train_loss, val_loss, train_acc, val_acc = train_model(\n",
        "    model50, criterion, optimizer, train_loader, val_loader, scheduler, num_epochs=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de resultados\n",
        "plot_training(train_loss, val_loss, train_acc, val_acc)"
      ],
      "metadata": {
        "id": "FJO1mM7-q4Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm6D7GC_-eIH"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explicabilidad"
      ],
      "metadata": {
        "id": "SVi4E-YT4lBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionamos 5 imagenes aleatorias de una clase\n",
        "class_name = \"010.Red_winged_Blackbird\"\n",
        "class_path = os.path.join(data_path, class_name)\n",
        "class_images = [os.path.join(class_path, img) for img in os.listdir(class_path)]\n",
        "selected_images = random.sample(class_images, 5)"
      ],
      "metadata": {
        "id": "ER1RHKSC4m7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torchcam.methods import GradCAM\n",
        "from torchcam.utils import overlay_mask\n",
        "\n",
        "# Transformación fija (sin randomizaciones)\n",
        "fixed_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "cam_extractor = GradCAM(model, target_layer=\"layer3\")\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "for idx, path_img in enumerate(selected_images):\n",
        "    image = Image.open(path_img).convert(\"RGB\")\n",
        "    input_tensor = fixed_transform(image).unsqueeze(0).to(device)\n",
        "    input_tensor.requires_grad_()\n",
        "\n",
        "    with torch.set_grad_enabled(True):\n",
        "        output = model(input_tensor)\n",
        "        class_idx = output.argmax().item()\n",
        "\n",
        "        # cam: ya viene en forma [1, 14, 14]\n",
        "        cam = cam_extractor(class_idx, output)[0]  # [1, 14, 14]\n",
        "\n",
        "        # Redimensionar (ya tiene forma [1, H, W] → añadimos batch dim)\n",
        "        cam_resized = F.interpolate(cam.unsqueeze(0), size=(224, 224), mode=\"bilinear\", align_corners=False)[0]\n",
        "\n",
        "        # Normalizar al rango [0,1]\n",
        "        cam_resized = cam_resized - cam_resized.min()\n",
        "        cam_resized = cam_resized / (cam_resized.max() + 1e-8)\n",
        "\n",
        "        # Convertir a PIL\n",
        "        cam_img = to_pil_image(cam_resized.cpu())\n",
        "\n",
        "        # Superponer heatmap sobre la imagen original redimensionada\n",
        "        result = overlay_mask(image.resize((224, 224)), cam_img, alpha=0.5)\n",
        "\n",
        "    plt.subplot(1, 5, idx + 1)\n",
        "    plt.imshow(result)\n",
        "    plt.title(f\"Pred: {dataset.classes[class_idx]}\", fontsize=9)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(f\"Grad-CAM sobre imágenes aleatorias de la clase: {class_name}\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rpok0jKc46_K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "2_22zgk8wGV8"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}